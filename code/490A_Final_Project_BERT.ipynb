{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "490A Final Project BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f"
      },
      "source": [
        "!pip install transformers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJEnBJ3gHTsQ"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiWulnsz7yWJ"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkTkdwAjy8p1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "\n",
        "annotations_aggregated = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/pfizer_processed.csv\")\n",
        "a_agg = annotations_aggregated['Volatility']\n",
        "pfizer = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/pfizer_processed.csv\").fillna(0)\n",
        "tesla = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/tesla_processed.csv\").fillna(0)\n",
        "dataset = pd.concat((pfizer, tesla), axis=0)\n",
        "\n",
        "\n",
        "X_total = dataset['Text']\n",
        "y_total = dataset['Volatility']\n",
        "y_total = np.array([i for i in y_total])\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r5Qn9rdSHUY"
      },
      "source": [
        "sum(y_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YslHyjop73Z0"
      },
      "source": [
        "### Generate Sentence Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcvzDtPrAzym"
      },
      "source": [
        "kf = KFold(random_state=24, shuffle=True, n_splits=5)\n",
        "f1_array = []\n",
        "precision_array = []\n",
        "recall_array = []\n",
        "acc_array = []\n",
        "lr = None\n",
        "fold = 1\n",
        "\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "total_tokens = list(map(lambda t: tokenizer.tokenize(\"[CLS] \" + t + \" [SEP]\"), X_total))\n",
        "print(len(total_tokens))\n",
        "\n",
        "total_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, total_tokens))\n",
        "total_segment_ids = list(map(lambda t, y: [y] * len(t), total_tokens, y_total))\n",
        "\n",
        "total_token_tensor_arr = list(map(lambda t: torch.tensor([t]), total_tokens_ids))\n",
        "total_segment_tensor_arr = list(map(lambda t: torch.tensor([t]), total_segment_ids))\n",
        "\n",
        "print(f\"train_tokens = {len(total_tokens)}\")\n",
        "print(f\"train_tokens_ids = {len(total_tokens_ids)}\")\n",
        "print(f\"train_segment_ids = {len(total_segment_ids)}\")\n",
        "print(f\"train_token_tensor_arr = {len(total_token_tensor_arr)}\")\n",
        "print(f\"train_segment_tensor_arr = {len(total_segment_tensor_arr)}\")\n",
        "with torch.no_grad():\n",
        "  total_outputs_arr = list(map(lambda x, y: model(x, y), total_token_tensor_arr, total_segment_tensor_arr))\n",
        "  total_hidden_states_arr = list(map(lambda t: t[2], total_outputs_arr))\n",
        "  \n",
        "total_token_vecs_arr = list(map(lambda t: t[-2][0], total_hidden_states_arr))\n",
        "total_sentence_embeddings_arr = list(map(lambda t: torch.mean(t, dim=0), total_token_vecs_arr))\n",
        "X_features = np.array([i.numpy() for i in total_sentence_embeddings_arr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4cbgDfx7-Qd"
      },
      "source": [
        "### Logistic Regression + BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34LakKhVs42d"
      },
      "source": [
        "def runLR(X_features, y_totl):\n",
        "    kf = KFold(random_state=24, shuffle=True, n_splits=5)\n",
        "    f1_array = []\n",
        "    precision_array = []\n",
        "    recall_array = []\n",
        "    acc_array = []\n",
        "    lr = None\n",
        "    fold = 1\n",
        "    y_total = np.array([i+1 for i in y_totl])\n",
        "    for train_index, test_index in kf.split(X_features):\n",
        "        print(f\"\\n_____fold = {fold}_____\")\n",
        "        X_train, X_test = X_features[train_index], X_features[test_index]\n",
        "        y_train, y_test = y_total[train_index], y_total[test_index]\n",
        "        print(f\"train = x:{X_train.shape}, y:{y_train.shape}\")\n",
        "\n",
        "        \n",
        "        lr = LogisticRegression(max_iter=1000)\n",
        "        lr.fit(X_train, y_train)\n",
        "        \n",
        "        y_pred = lr.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "        f1 = f1_score(y_test, y_pred, average='binary')\n",
        "        prec = precision_score(y_test, y_pred)\n",
        "        f1_array.append(f1)\n",
        "        precision_array.append(prec)\n",
        "        rec = recall_score(y_test, y_pred)\n",
        "        recall_array.append(rec)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        acc_array.append(acc)\n",
        "        \n",
        "        print(f\"F1 score\\t = {f1}\")\n",
        "        print(f\"precision score = {prec}\")\n",
        "        print(f\"recall score = {rec}\")\n",
        "        print(f\"accuracy score = {acc}\")\n",
        "        \n",
        "        fold+=1\n",
        "\n",
        "    print(\"\\n__________________\\n\\naverage f1 score over all folds = \" + str(np.mean([f1_array])))\n",
        "    print(\"average precision over all folds\\t = \"+ str(np.mean([precision_array])))\n",
        "    print(\"average recall over all folds\\t\\t = \" + str(np.mean([recall_array])))\n",
        "    print(\"average accuracy over all folds\\t\\t = \" + str(np.mean([acc_array])))\n",
        "  \n",
        "runLR(X_features, y_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWcLwZjXI_1c"
      },
      "source": [
        "### Neural Network + BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDhnlRZT2Fxj"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.utils.data as utils\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import  train_test_split as train_test_split\n",
        "from IPython.display import display, HTML\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class SeqModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features, num_targets, batch_size):\n",
        "        super().__init__()\n",
        "        self.layer1_size = 32\n",
        "        self.layer2_size = 16\n",
        "        self.layer3_size = 2\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.max_grad_norm = 1000\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.num_targets = num_targets\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.inputnorm = nn.BatchNorm1d(self.num_features)\n",
        "        \n",
        "        self.layer1 = nn.Linear(num_features, self.layer1_size)\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "        self.prelu = nn.PReLU()\n",
        "        \n",
        "        self.hiddenlinear = nn.Linear(self.layer1_size, self.layer2_size)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        \n",
        "        self.layer3 = nn.Linear(self.layer2_size, self.layer3_size)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "        \n",
        " \n",
        "        \n",
        "        self.final = nn.Linear(self.layer3_size, num_targets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.prelu(x)\n",
        "        x = self.hiddenlinear(x)\n",
        "        x = self.prelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.prelu(x)\n",
        "        x = self.final(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_mdl(model, device, train_loader, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        loss = nn.BCEWithLogitsLoss()(output, target)\n",
        "        if model.gradient_accumulation_steps > 1:\n",
        "            loss = loss / model.gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), model.max_grad_norm)\n",
        "        if (batch_idx + 1) % model.gradient_accumulation_steps == 0:\n",
        "            scheduler.step()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        if batch_idx % 100 == 0: #Print loss every 100 batch\n",
        "            print('Train Epoch: {}\\tLoss: {:.6f}'.format(\n",
        "                epoch, loss.item()))\n",
        "    Loss = test_mdl(model, device, train_loader)\n",
        "    return Loss\n",
        "\n",
        "def test_mdl(model, device, test_loader):\n",
        "    model.eval()\n",
        "    loss_arr = []\n",
        "    acc_arr = []\n",
        "    prec_arr = []\n",
        "    recall_arr = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            p = []\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = nn.BCEWithLogitsLoss()(output, target)\n",
        "            loss_arr.append(loss.item())\n",
        "            p.append([output.sigmoid().cpu().numpy()])\n",
        "            probas = np.array(p).reshape(-1)\n",
        "            preds = [1 if i>0.5 else 0 for i in probas]\n",
        "            acc_arr.append(accuracy_score(target.cpu().numpy(), preds))\n",
        "            prec_arr.append(precision_score(target.cpu().numpy(), preds))\n",
        "            recall_arr.append(recall_score(target.cpu().numpy(), preds))\n",
        "    print(f\"acc = {np.mean(acc_arr)}\")\n",
        "    print(f\"prec = {np.mean(prec_arr)}\")\n",
        "    print(f\"recall = {np.mean(recall_arr)}\")\n",
        "    return np.mean(loss_arr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main2(X, Y):\n",
        "    train_data , valid_data, train_target, valid_target = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
        "\n",
        "    use_cuda= True\n",
        "    learning_rate = 0.0001\n",
        "    NumEpochs = 15\n",
        "    batch_size = 16\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    tensor_x = torch.tensor(train_data,dtype=torch.float, device=device)\n",
        "    tensor_y = torch.tensor(train_target,dtype=torch.float,  device=device)\n",
        "\n",
        "    test_tensor_x = torch.tensor(valid_data,dtype=torch.float, device=device)\n",
        "    test_tensor_y = torch.tensor(valid_target,dtype=torch.float)\n",
        "    \n",
        "    train_dataset = utils.TensorDataset(tensor_x, tensor_y)  # create your datset\n",
        "    train_loader = utils.DataLoader(train_dataset, batch_size=batch_size, drop_last=True)  # create your dataloader\n",
        "    \n",
        "    test_dataset = utils.TensorDataset(test_tensor_x, test_tensor_y)  # create your datset\n",
        "    test_loader = utils.DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "\n",
        "    model = SeqModel(768, 1, batch_size)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adamax(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, max_lr=1e-2, epochs=NumEpochs, steps_per_epoch=len(train_loader))\n",
        "\n",
        "\n",
        "    train_Loss_list = []\n",
        "    test_Loss_list = []\n",
        "    epoch_list = []\n",
        "    for epoch in range(NumEpochs):\n",
        "        epoch_list.append(epoch)\n",
        "        train_loss = train_mdl(model, device, train_loader, optimizer, scheduler, epoch)\n",
        "        train_Loss_list.append(train_loss)\n",
        "        print(f'\\nTrain set Loss: {train_loss}')\n",
        "        test_loss = test_mdl(model, device, test_loader)\n",
        "        print(f'Test set Loss: {test_loss}\\n')\n",
        "        test_Loss_list.append(test_loss)\n",
        "\n",
        "    # Plot train and test accuracy vs epoch\n",
        "    plt.figure(\"Train and Test Loss vs Epoch\")\n",
        "    plt.plot(epoch_list, train_Loss_list, c='r', label=\"Train Loss\")\n",
        "    plt.plot(epoch_list, test_Loss_list, c='g', label=\"Test Loss\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Number of Epochs\")\n",
        "    plt.legend(loc=0)\n",
        "    plt.show()\n",
        "    return model\n",
        "\n",
        "y_target = y_total.reshape(-1, 1)\n",
        "\n",
        "X_train_f, X_valid, y_train_f, y_valid = train_test_split(X_features, y_target,test_size=0.5, random_state=42)\n",
        "\n",
        "mdl = main2(X_train_f, y_train_f)\n",
        "testvals = torch.tensor(X_valid,dtype=torch.float,  device=torch.device(\"cuda\"))\n",
        "p = mdl(testvals).sigmoid().detach().cpu().numpy()\n",
        "probas = np.array(p).reshape(-1)\n",
        "preds = [1 if i>=0.5 else 0 for i in probas]\n",
        "print(f\"test accuracy = {accuracy_score(y_valid, preds)}\")\n",
        "print(f\"test f1 = {f1_score(y_valid, preds, average='binary')}\")\n",
        "print(f\"test precision = {precision_score(y_valid, preds)}\")\n",
        "print(f\"test recall = {recall_score(y_valid, preds)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}